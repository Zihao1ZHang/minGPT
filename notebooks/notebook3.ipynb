{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cikm2021-tutorial-part3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9egyhuvU3_GI"
      },
      "source": [
        "# PyTerrier CIKM 2021 Tutorial Notebook - Part 3 - Neural Re-Ranking and Neural Index Augmentation\n",
        "\n",
        "This is one of a series of Colab notebooks created for the [CIKM 2021](https://www.cikm2021.org/) Tutorial entitled '**IR From Bag-of-words to BERT and Beyond through Practical Experiments**'. It demonstrates the use of [PyTerrier](https://github.com/terrier-org/pyterrier) on the [CORD19 test collection](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge).\n",
        "\n",
        "In particular, in this notebook you will:\n",
        "\n",
        " - Re-rank documents using neural models like KNRM, Vanilla BERT, EPIC, and monoT5.\n",
        " - Use DeepCT and doc2query to augment documents for lexical retrieval functions like BM25."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl0-Gs6e5I7n"
      },
      "source": [
        "## Setup\n",
        "\n",
        "In the following, we will set up the libraries required to execute the notebook.\n",
        "\n",
        "### Pyterrier installation\n",
        "\n",
        "The following cell installs the latest release of the [PyTerrier](https://github.com/terrier-org/pyterrier) package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSgDzjKxqfq5",
        "outputId": "e46aaaa2-9721-4259-f765-f05b3bc87ac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -q --upgrade python-terrier"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.4/163.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.0/859.0 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.0/288.0 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.2/119.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for chest (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV0C6jJvqhMR"
      },
      "source": [
        "### Pyterrier plugins installation\n",
        "\n",
        "We install the [OpenNIR](https://opennir.net/), [monoT5](https://github.com/terrierteam/pyterrier_t5), [DeepCT](https://github.com/terrierteam/pyterrier_deepct) and [doc2query](https://github.com/terrierteam/pyterrier_doc2query) PyTerrier plugins. You can safely ignore the package versioning errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkIR_PXdet7R",
        "outputId": "a4512340-6d49-4ad1-b127-a28618d02281",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -q --upgrade git+https://github.com/Georgetown-IR-Lab/OpenNIR\n",
        "!pip install -q --upgrade git+https://github.com/terrierteam/pyterrier_t5\n",
        "!pip install -q --upgrade git+https://github.com/terrierteam/pyterrier_deepct.git\n",
        "!pip install -q --upgrade git+https://github.com/terrierteam/pyterrier_doc2query.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.3/114.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m842.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m128.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for OpenNIR (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyterrier-t5 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyterrier-deepct (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for DeepCT (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyterrier-doc2query (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-nQrpNP5pN7"
      },
      "source": [
        "## Preliminary steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSUwC6S7QkQY"
      },
      "source": [
        "**[PyTerrier](https://github.com/terrier-org/pyterrier) initialization**\n",
        "\n",
        "Lets get [PyTerrier](https://github.com/terrier-org/pyterrier) started. This will download the latest version of the [Terrier](http://terrier.org/) IR platform. We also import the [OpenNIR](https://opennir.net/) pyterrier bindings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FegcyWr5lja"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple, Set\n",
        "class FIQSDataLoader:\n",
        "    \"\"\"\n",
        "    A data loader for the FIQS dataset that handles loading and processing of:\n",
        "    - corpus.jsonl (document collection)\n",
        "    - queries.jsonl (queries)\n",
        "    - train.tsv, test.tsv, dev.tsv (relevance judgments)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_dir: str = \".\"):\n",
        "        \"\"\"\n",
        "        Initialize the data loader with the directory containing the FIQS files.\n",
        "\n",
        "        Args:\n",
        "            data_dir: Path to the directory containing the FIQS files\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.corpus = {}\n",
        "        self.queries = {}\n",
        "        self.train_data = None\n",
        "        self.test_data = None\n",
        "        self.dev_data = None\n",
        "\n",
        "    def load_corpus(self, filepath: str = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Load the corpus from a JSONL file.\n",
        "\n",
        "        Args:\n",
        "            filepath: Path to the corpus file. If None, uses default location.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping document IDs to document objects\n",
        "        \"\"\"\n",
        "        if filepath is None:\n",
        "            filepath = f\"{self.data_dir}/corpus.jsonl\"\n",
        "\n",
        "        corpus = {}\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if line.strip():  # Skip empty lines\n",
        "                    doc = json.loads(line)\n",
        "                    corpus[doc[\"_id\"]] = doc\n",
        "\n",
        "        self.corpus = corpus\n",
        "        return corpus\n",
        "\n",
        "    def load_queries(self, filepath: str = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Load the queries from a JSONL file.\n",
        "\n",
        "        Args:\n",
        "            filepath: Path to the queries file. If None, uses default location.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping query IDs to query objects\n",
        "        \"\"\"\n",
        "        if filepath is None:\n",
        "            filepath = f\"{self.data_dir}/queries.jsonl\"\n",
        "\n",
        "        queries = {}\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if line.strip():  # Skip empty lines\n",
        "                    query = json.loads(line)\n",
        "                    queries[query[\"_id\"]] = query\n",
        "\n",
        "        self.queries = queries\n",
        "        return queries\n",
        "\n",
        "    def load_relevance_judgments(self, split: str = \"train\", filepath: str = None) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load relevance judgments from a TSV file.\n",
        "\n",
        "        Args:\n",
        "            split: Which split to load ('train', 'test', or 'dev')\n",
        "            filepath: Path to the TSV file. If None, uses default location.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame containing the relevance judgments\n",
        "        \"\"\"\n",
        "        if filepath is None:\n",
        "            filepath = f\"{self.data_dir}/{split}.tsv\"\n",
        "\n",
        "        # Load the TSV file\n",
        "        df = pd.read_csv(filepath, sep='\\t')\n",
        "\n",
        "        # Store the data in the appropriate attribute\n",
        "        if split == \"train\":\n",
        "            self.train_data = df\n",
        "        elif split == \"test\":\n",
        "            self.test_data = df\n",
        "        elif split == \"dev\":\n",
        "            self.dev_data = df\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown split: {split}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def load_all_data(self) -> Tuple[Dict, Dict, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Load all FIQS data files.\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (corpus, queries, train_data, test_data, dev_data)\n",
        "        \"\"\"\n",
        "        corpus = self.load_corpus()\n",
        "        queries = self.load_queries()\n",
        "        train_data = self.load_relevance_judgments(\"train\")\n",
        "        test_data = self.load_relevance_judgments(\"test\")\n",
        "        dev_data = self.load_relevance_judgments(\"dev\")\n",
        "\n",
        "        return corpus, queries, train_data, test_data, dev_data\n",
        "\n",
        "    def get_relevant_docs_for_query(self, query_id: str, split: str = \"train\") -> Set[str]:\n",
        "        \"\"\"\n",
        "        Get the IDs of documents that are relevant to a specific query.\n",
        "\n",
        "        Args:\n",
        "            query_id: ID of the query\n",
        "            split: Which split to use ('train', 'test', or 'dev')\n",
        "\n",
        "        Returns:\n",
        "            Set of relevant document IDs\n",
        "        \"\"\"\n",
        "        if split == \"train\":\n",
        "            if self.train_data is None:\n",
        "                self.load_relevance_judgments(\"train\")\n",
        "            df = self.train_data\n",
        "        elif split == \"test\":\n",
        "            if self.test_data is None:\n",
        "                self.load_relevance_judgments(\"test\")\n",
        "            df = self.test_data\n",
        "        elif split == \"dev\":\n",
        "            if self.dev_data is None:\n",
        "                self.load_relevance_judgments(\"dev\")\n",
        "            df = self.dev_data\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown split: {split}\")\n",
        "\n",
        "        # Filter the DataFrame for the given query_id\n",
        "        relevant_docs_df = df[df[\"query-id\"] == query_id]\n",
        "\n",
        "        # Extract corpus IDs and convert to string for consistent comparison\n",
        "        relevant_doc_ids = set(str(doc_id) for doc_id in relevant_docs_df[\"corpus-id\"].tolist())\n",
        "\n",
        "        return relevant_doc_ids\n",
        "\n",
        "    def get_relevant_doc_dict(self, split: str = \"train\") -> Dict[str, Set[str]]:\n",
        "        \"\"\"\n",
        "        Get a dictionary mapping query IDs to sets of relevant document IDs.\n",
        "\n",
        "        Args:\n",
        "            split: Which split to use ('train', 'test', or 'dev')\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping query IDs to sets of relevant document IDs\n",
        "        \"\"\"\n",
        "        result = {}\n",
        "        for query_id in self.queries.keys():\n",
        "            try:\n",
        "                relevant_docs = self.get_relevant_docs_for_query(query_id, split)\n",
        "                if relevant_docs:  # Only include queries with relevant docs\n",
        "                    result[query_id] = relevant_docs\n",
        "            except:\n",
        "                # Skip queries that don't have relevance judgments in this split\n",
        "                continue\n",
        "        return result"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = FIQSDataLoader()\n",
        "corpus, queries, train, test_data, dev_data = loader.load_all_data()"
      ],
      "metadata": {
        "id": "2AxUW3s-bRFO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPVjr448rIPc"
      },
      "source": [
        "### [TREC-COVID19](https://ir.nist.gov/covidSubmit/) Dataset download\n",
        "\n",
        "The following cell downloads the [TREC-COVID19](https://ir.nist.gov/covidSubmit/) dataset that we will use in the remainder of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJMHFRfArd7O"
      },
      "source": [
        "dataset = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
        "topics = dataset.get_topics(variant='description')\n",
        "qrels = dataset.get_qrels()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF3HIPhtrqOH"
      },
      "source": [
        "### Terrier inverted index download\n",
        "\n",
        "To save a few minutes, we use a pre-built Terrier inverted index for the TREC-COVID19 collection ([`'terrier_stemmed'`](http://data.terrier.org/trec-covid.dataset.html#terrier_stemmed) version). Download time took a few seconds for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oCcP90yrlGi"
      },
      "source": [
        "index = pt.get_dataset('trec-covid').get_index('terrier_stemmed_positions')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwDams5M7g6c"
      },
      "source": [
        "## Re-Rankers from scratch\n",
        "\n",
        "Let's start exploring a few neural re-ranking methods! We can build them from scratch using `onir_pt.reranker`.\n",
        "\n",
        "And OpenNIR reranking model consists of:\n",
        " - `ranker` (e.g., `drmm`, `knrm`, or `pacrr`). This defines the neural ranking architecture.\n",
        " - `vocab` (e.g., `wordvec_hash`, or `bert`). This defines how text is encoded by the model. This approach makes it easy to swap out different text representations.\n",
        "\n",
        "This line will take a few minutes to run as it downloads and prepares the word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0O79K2K6fvn"
      },
      "source": [
        "knrm = onir_pt.reranker('knrm', 'wordvec_hash', text_field='title_abstract')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1avVTpxDORN"
      },
      "source": [
        "Let's look at how well these models work at ranking!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FWUIN577v1O"
      },
      "source": [
        "br = pt.BatchRetrieve(index) % 50\n",
        "# build a sub-pipeline to get the concatenated title and abstract text\n",
        "get_title_abstract = pt.text.get_text(dataset, 'title') >> pt.text.get_text(dataset, 'abstract') >> pt.apply.title_abstract(lambda r: r['title'] + ' ' + r['abstract'])\n",
        "pipeline = br >> get_title_abstract >> knrm\n",
        "pt.Experiment(\n",
        "    [br, pipeline],\n",
        "    topics,\n",
        "    qrels,\n",
        "    names=['DPH', 'DPH >> KNRM'],\n",
        "    eval_metrics=[AP(rel=2), nDCG, nDCG@10, P(rel=2)@10]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhquobQypVNJ"
      },
      "source": [
        "This doesn't work very well because the model is not trained; it's using random weights to combine the scores from the similarity matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLv7quA43yAP"
      },
      "source": [
        "## Loading a trained re-ranker\n",
        "\n",
        "You can train re-ranking models in PyTerrier using the `fit` method. This takes a bit of time, so we'll download a model that's already been trained. If you'd like to train the model yourself, you can use:\n",
        "\n",
        "```python\n",
        "# transfer training signals from a medical sample of MS MARCO\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_ds = pt.datasets.get_dataset('irds:msmarco-passage/train/medical')\n",
        "train_topics, valid_topics = train_test_split(train_ds.get_topics(), test_size=50, random_state=42) # split into training and validation sets\n",
        "\n",
        "# Index MS MARCO\n",
        "indexer = pt.index.IterDictIndexer('./terrier_msmarco-passage')\n",
        "tr_index_ref = indexer.index(train_ds.get_corpus_iter(), fields=('text',), meta=('docno',))\n",
        "\n",
        "pipeline = (pt.BatchRetrieve(tr_index_ref) % 100 # get top 100 results\n",
        "            >> pt.text.get_text(train_ds, 'text') # fetch the document text\n",
        "            >> pt.apply.generic(lambda df: df.rename(columns={'text': 'abstract'})) # rename columns\n",
        "            >> knrm) # apply neural re-ranker\n",
        "\n",
        "pipeline.fit(\n",
        "    train_topics,\n",
        "    train_ds.get_qrels(),\n",
        "    valid_topics,\n",
        "    train_ds.get_qrels())\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk7FBOgvDa8V"
      },
      "source": [
        "del knrm # free up the memory before loading a new version of the ranker\n",
        "knrm = onir_pt.reranker.from_checkpoint('https://macavaney.us/knrm.medmarco.tar.gz', text_field='title_abstract', expected_md5=\"d70b1d4f899690dae51161537e69ed5a\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BQQKv8lL0Ta"
      },
      "source": [
        "pipeline = br >> get_title_abstract >> knrm\n",
        "pt.Experiment(\n",
        "    [br, pipeline],\n",
        "    topics,\n",
        "    qrels,\n",
        "    names=['DPH', 'DPH >> KNRM'],\n",
        "    baseline=0,\n",
        "    eval_metrics=[AP(rel=2), nDCG, nDCG@10, P(rel=2)@10]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI1_8O8rtXKK"
      },
      "source": [
        "That's a little better than before, but it still underperforms our first-stage ranking model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79l1jn0pRQEY"
      },
      "source": [
        "## Vanilla BERT\n",
        "\n",
        "Contextualized language models, such as [BERT](https://arxiv.org/abs/1810.04805), are much more powerful neural models that have been shown to be effective for ranking.\n",
        "\n",
        "We'll try using a \"vanilla\" (or \"mono\") version of the BERT model. The BERT model is pre-trained for the task of language modeling and next sentence prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qlXPHqN3iO0"
      },
      "source": [
        "del knrm # clear out memory from KNRM\n",
        "vbert = onir_pt.reranker('vanilla_transformer', 'bert', text_field='title_abstract', vocab_config={'train': True})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "progrVwaunrn"
      },
      "source": [
        "Let's see how this model does on TREC COVID."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkasovrjQjy0"
      },
      "source": [
        "pipeline = br % 50 >> get_title_abstract >> vbert\n",
        "pt.Experiment(\n",
        "    [br, pipeline],\n",
        "    topics,\n",
        "    qrels,\n",
        "    names=['DPH', 'DPH >> VBERT'],\n",
        "    baseline=0,\n",
        "    eval_metrics=[AP(rel=2), nDCG, nDCG@10, P(rel=2)@10]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBrfNZ_1u_pD"
      },
      "source": [
        "As we see, although the model is pre-trained, it doesn't do very well at ranking on our benchmark. This is because it's not tuned for the task of relevance ranking.\n",
        "\n",
        "We can train the model for ranking (as shown above for KNRM) or we can download a trained model. Here, we use the [SLEDGE](https://arxiv.org/abs/2010.05987) model, which is a Vanilla BERT model trained on scientific text and tuned on medical queries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsXQKNyYSXOj"
      },
      "source": [
        "sledge = onir_pt.reranker.from_checkpoint('https://macavaney.us/scibert-medmarco.tar.gz', text_field='title_abstract', expected_md5=\"854966d0b61543ffffa44cea627ab63b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUH-daNJSoNy"
      },
      "source": [
        "pipeline = br % 50 >> get_title_abstract >> sledge\n",
        "pt.Experiment(\n",
        "    [br, pipeline],\n",
        "    topics,\n",
        "    qrels,\n",
        "    names=['DPH', 'DPH >> SLEDGE'],\n",
        "    baseline=0,\n",
        "    eval_metrics=[AP(rel=2), nDCG, nDCG@10, P(rel=2)@10, 'mrt']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtAxDQHyv4ON"
      },
      "source": [
        "That's much better! We're able to significantly improve upon the first stage ranker. But we can see that this is pretty slow to run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRcBOIBPRJre"
      },
      "source": [
        "## EPIC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef4O3PyAHMuS"
      },
      "source": [
        "Some models focus on query-time computational efficiency. The [EPIC](https://arxiv.org/abs/2004.14245) model builds light-weight document representations that are independent of the query. This means that they can be computed ahead of time. You can index the corpus yourself with the following code (but it takes a while):\n",
        "\n",
        "```python\n",
        "indexed_epic = onir_pt.indexed_epic.from_checkpoint('https://macavaney.us/epic.msmarco.tar.gz', index_path='./epic_cord19')\n",
        "indexed_epic.index(dataset.get_corpus_iter(), fields=('title', 'abstract'))\n",
        "```\n",
        "\n",
        "Instead, we'll download a copy of the EPIC-processed documents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwkE23yfGl65"
      },
      "source": [
        "import os\n",
        "if not os.path.exists('epic_cord19.zip'):\n",
        "  !wget http://macavaney.us/epic_cord19.zip\n",
        "  !unzip epic_cord19.zip\n",
        "indexed_epic = onir_pt.indexed_epic.from_checkpoint('https://macavaney.us/epic.msmarco.tar.gz', index_path='./epic_cord19')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7LfbxJqw80B"
      },
      "source": [
        "We can now run this model over the results of a first-stage ranker. Note how we do not need to fetch the document text with `pt.text.get_text`, which further saves time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tynDMGCYJCUv"
      },
      "source": [
        "br = pt.BatchRetrieve(index) % 50\n",
        "pipeline = (br >> indexed_epic.reranker())\n",
        "pt.Experiment(\n",
        "    [br, pipeline],\n",
        "    dataset.get_topics('description'),\n",
        "    dataset.get_qrels(),\n",
        "    names=['DPH', 'DPH >> EPIC (indexed)'],\n",
        "    eval_metrics=[AP(rel=2), nDCG, nDCG@10, P(rel=2)@10, \"mrt\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCF-DglUPrB-"
      },
      "source": [
        "## Tuning re-ranking threshold\n",
        "\n",
        "[Prior work suggests](https://arxiv.org/pdf/1904.12683.pdf) that the re-ranking cutoff threshold is an important model hyperparameter. Let's see how this parameter affects EPIC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osnHkyGtNy1m"
      },
      "source": [
        "cutoffs = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "dph = pt.BatchRetrieve(index)\n",
        "res = pt.Experiment(\n",
        "    [dph % cutoff >> indexed_epic.reranker() for cutoff in cutoffs],\n",
        "    dataset.get_topics('description'),\n",
        "    dataset.get_qrels(),\n",
        "    names=[f'c={cutoff}' for cutoff in cutoffs],\n",
        "    eval_metrics=[AP(rel=2), nDCG, nDCG@10, P(rel=2)@10, \"mrt\"]\n",
        ")\n",
        "res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvq1gw8VO_rp"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(res['name'], res['nDCG@10'], label='nDCG@10')\n",
        "plt.plot(res['name'], res['P(rel=2)@10'], label='P(rel=2)@10')\n",
        "plt.ylabel('value')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.clf()\n",
        "plt.plot(res['name'], res['mrt'])\n",
        "plt.ylabel('mrt')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8tu_yN4QRdh"
      },
      "source": [
        "It appears that the optimal re-ranking threshold for this collection is around 50-70. This also avoids excessive re-ranking time, which grows roughly linearly with larger thredhols. In pratice, this paramter should be tuned on a held-out validation set to avoid over-fitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdUPsiYLX-L1"
      },
      "source": [
        "## monoT5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1UoVYscxzR_"
      },
      "source": [
        "The [monoT5](https://arxiv.org/abs/2003.06713) model scores documents using a causal language model. Let's see how this approach works on TREC COVID.\n",
        "\n",
        "The `MonoT5ReRanker` class from `pyterrier_t5` automatically loads a version of the monoT5 ranker that is trained on the MS MARCO passage dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuWuB44kQN2f"
      },
      "source": [
        "from pyterrier_t5 import MonoT5ReRanker\n",
        "monoT5 = MonoT5ReRanker(text_field='title_abstract')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FI_KCicWDeB"
      },
      "source": [
        "br = pt.BatchRetrieve(index) % 50\n",
        "pipeline = (br >> get_title_abstract >> monoT5)\n",
        "pt.Experiment(\n",
        "    [br, pipeline],\n",
        "    dataset.get_topics('description'),\n",
        "    dataset.get_qrels(),\n",
        "    names=['DPH', 'DPH >> T5'],\n",
        "    eval_metrics=[AP(rel=2), nDCG, nDCG@10, P(rel=2)@10, \"mrt\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP3KpU8t2-Wv"
      },
      "source": [
        "## DeepCT\n",
        "\n",
        "Recall that the DeepCT model repeats terms based on their estimated importance. This repitition boosts the importance in an inverted index structure.\n",
        "\n",
        "We provide an interface to the DeepCT model in the `pyterrier_deepct` package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZiZBUJ03ZNH"
      },
      "source": [
        "import pyterrier_deepct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC1Vr5sC3acI"
      },
      "source": [
        "### Loading a pre-trained model\n",
        "\n",
        "We will load the pre-trained verison of DeepCT provided by the authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1tIMtO73aF7"
      },
      "source": [
        "if not os.path.exists(\"marco.zip\"):\n",
        "  !wget http://boston.lti.cs.cmu.edu/appendices/arXiv2019-DeepCT-Zhuyun-Dai/outputs/marco.zip\n",
        "  !unzip marco.zip\n",
        "if not os.path.exists(\"uncased_L-12_H-768_A-12.zip\"):\n",
        "  !wget https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-768_A-12.zip\n",
        "  !unzip uncased_L-12_H-768_A-12.zip\n",
        "  !mkdir -p bert-base-uncased\n",
        "  !mv vocab.txt bert_* bert-base-uncased/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxoe3V0A3i_c"
      },
      "source": [
        "Loading a model is as simple as specifying the model configuration and weight file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGq4oPWl3h9I"
      },
      "source": [
        "deepct = pyterrier_deepct.DeepCTTransformer(\"bert-base-uncased/bert_config.json\", \"marco/model.ckpt-65816\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unAhL8fF3lMT"
      },
      "source": [
        "### Running on sample text\n",
        "\n",
        "We can transform a dataframe with a sample document to observe the effect of DeepCT:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFc3xjf03oVg"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame([{\"docno\" : \"d1\", \"text\" :\"The 30th ACM International Conference on Information and Knowledge Management (CIKM) is held virtually due to the COVID-19 pandemic.\"}])\n",
        "df.iloc[0].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1mTKlHx3oTV"
      },
      "source": [
        "deepct_df = deepct(df)\n",
        "deepct_df.iloc[0].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUxtVJDJ34u3"
      },
      "source": [
        "(You may need to expand the text using the \\[...\\] button at the end of the text.)\n",
        "\n",
        "Interesting, right? We can see a lot of terms are expanded. Let's use `Counter` to see which are the most important terms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9GSETVW37Z5"
      },
      "source": [
        "from collections import Counter\n",
        "Counter(deepct_df.iloc[0].text.split()).most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg6x7VKo3-mK"
      },
      "source": [
        "As you can see, DeepCT considers \"Conference\", \"CIKM\", and \"ACM\" to be the most important terms in the document. Not bad choices. However, it completley removes the word \"virtually\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW35rEdQ4Bxs"
      },
      "source": [
        "### Loading an index of DeepCT documents\n",
        "\n",
        "It takes too long to run DeepCT over the entire CORD19 collection in a tutorial setting, so we provide a version of the index for download.\n",
        "\n",
        "If you would like to index the collection with DeepCT yourself, you can use:\n",
        "\n",
        "```python\n",
        "dataset = pt.get_dataset(\"irds:cord19/trec-covid\")\n",
        "indexer = (\n",
        "  pt.apply.generic(lambda df: df.rename(columns={'abstract': 'text'})) # rename \"abstract\" column to \"text\"\n",
        "  >> deepct # apply DeepCT transformation\n",
        "  >> pt.IterDictIndexer(\"./deepct_index_path\")) # index the modified documents\n",
        "indexref = indexer.index(dataset.get_corpus_iter())\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlFLMgoh398T"
      },
      "source": [
        "if not os.path.exists('deepct_marco_cord19.zip'):\n",
        "  !wget http://www.dcs.gla.ac.uk/~craigm/cikm2021-tutorial/deepct_marco_cord19.zip\n",
        "  !unzip deepct_marco_cord19.zip\n",
        "deepct_indexref = pt.IndexRef.of('./deepct_index_path')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TarsGVzjaG6r"
      },
      "source": [
        "How well does DeepCT perform on TREC COVID? Let's run an experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNfxKuLCaKnS"
      },
      "source": [
        "pt.Experiment(\n",
        "    [br, pt.BatchRetrieve(deepct_indexref)],\n",
        "    dataset.get_topics('description'),\n",
        "    dataset.get_qrels(),\n",
        "    names=['DPH', 'DeepCT'],\n",
        "    baseline=0,\n",
        "    eval_metrics=[AP(rel=2), nDCG, nDCG@10, P(rel=2)@10]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1LbWpN66wDD"
      },
      "source": [
        "The recall improves DPH (so unbounded nDCG improves), but the top results suffer (nDCG@10 and P(rel=2)@10 are reduced). Let's dig into the top results for each of the TREC COVID quereis to see what's happening."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTsTApIm6wsJ"
      },
      "source": [
        "pipeline = pt.BatchRetrieve(deepct_indexref) % 1 >> pt.text.get_text(dataset, 'title')\n",
        "res = pipeline(topics)\n",
        "res.merge(qrels, how='left').head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aht6dwwo7CHU"
      },
      "source": [
        "Ouch-- queries 2, 3, and 4 are non-relevant (the top doc for query 1 wasn't judged). Let's dig deeper into those source documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT0Aw29r7CsL"
      },
      "source": [
        "df = pd.DataFrame(doc for doc in dataset.get_corpus_iter() if doc['docno'] in ('g8grcy5j', '2c4jk2ms', 'mtjs9zv9'))\n",
        "df = df.rename(columns={'abstract': 'text'})\n",
        "deepct_df = deepct(df)\n",
        "print('deepct-transformed documents')\n",
        "for deepct_text, docno, text in zip(deepct_df['text'], deepct_df['docno'], df['text']):\n",
        "  print(docno)\n",
        "  print(Counter(deepct_text.split()).most_common(10))\n",
        "  print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26NvE33k7Jg5"
      },
      "source": [
        "As we can see, the document ranked highest for \"*will sars cov2 infected people develop immunity*\" (2c4jk2ms) gives high scores to the term sars, overpowering the other query terms.\n",
        "\n",
        "The top document for \"*what causes death from covid 19*\" (mtjs9zv9) has high scores for covid, 19, and death, but is discussing the topic with respect to HIV rather than COVID itself. This underscores the limitations of using bag-of-words for scoring instead.\n",
        "\n",
        "The top document for \"*how does the coronavirus respond to changes in weather*\" (g8grcy5j) discusses the potential for change in climate policy as a result of COVID-19, not how the virus responds to weather. DeepCT picks up on this theme and gives weather-related words high importance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8KAxXQQbEDU"
      },
      "source": [
        "## doc2query\n",
        "\n",
        "Recall that doc2query augments an inverted index structure by predicting queries that may be used to search for the document, and appending those to the document text.\n",
        "\n",
        "We provide an interface to doc2query using the `pyterrier_doc2query` package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEvZKb-GbH6Q"
      },
      "source": [
        "import pyterrier_doc2query"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7xwmWR4bJcm"
      },
      "source": [
        "### Loading a pre-trained model\n",
        "\n",
        "We will again use a version of the doc2query model released by the authors that is trained on the MS MARCO collection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QME5bJRbMJ1"
      },
      "source": [
        "import os\n",
        "if not os.path.exists(\"t5-base.zip\"):\n",
        "  !wget https://git.uwaterloo.ca/jimmylin/doc2query-data/raw/master/T5-passage/t5-base.zip\n",
        "  !unzip t5-base.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV7uGpZybV0k"
      },
      "source": [
        "We can load the model weights by specifying the checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGGu4hm7bXtK"
      },
      "source": [
        "doc2query = pyterrier_doc2query.Doc2Query('model.ckpt-1004000', batch_size=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SlBjxC-baOY"
      },
      "source": [
        "### Running on sample text\n",
        "\n",
        "Let's see what queries it predicts for the sample document:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhNRK_k9bZVi"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame([{\"docno\" : \"d1\", \"text\" :\"The 30th ACM International Conference on Information and Knowledge Management (CIKM) is held virtually due to the COVID-19 pandemic.\"}])\n",
        "df.iloc[0].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZANhTSubjIF"
      },
      "source": [
        "doc2query_df = doc2query(df)\n",
        "doc2query_df.iloc[0].querygen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POWQLjE6bo1j"
      },
      "source": [
        "Doc2query can genrate some resonable questions (e.g., \"*where is cikm held*\"), but also can generates some that are off-topic and introduce some non-relevant terms (e.g., cicm, cmim)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kadv-X8OcEp8"
      },
      "source": [
        "### Loading an index of doc2query documents\n",
        "\n",
        "Let's see how it does on TREC COVID. Again, it takes too long to index in a tutorial setting, so we provide an index.\n",
        "\n",
        "If you would like to index the collection with doc2query yourself, you can use:\n",
        "\n",
        "```python\n",
        "dataset = pt.get_dataset(\"irds:cord19/trec-covid\")\n",
        "indexer = (\n",
        "  pyterrier_doc2query.Doc2Query('model.ckpt-1004000', doc_attr='abstract', batch_size=8, append=True) # aply doc2query on abstracts and append\n",
        "  >> pt.apply.generic(lambda df: df.rename(columns={'abstract': 'text'}) # rename \"abstract\" column to \"text\" for indexing\n",
        "  >> pt.IterDictIndexer(\"./doc2query_index_path\")) # index the expanded documents\n",
        "indexref = indexer.index(dataset.get_corpus_iter())\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uy7Bt-cbof2"
      },
      "source": [
        "if not os.path.exists('doc2query_marco_cord19.zip'):\n",
        "  !wget http://www.dcs.gla.ac.uk/~craigm/cikm2021-tutorial/doc2query_marco_cord19.zip\n",
        "  !unzip doc2query_marco_cord19.zip\n",
        "doc2query_indexref = pt.IndexRef.of('./doc2query_index_path')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NCSiTq6ckYe"
      },
      "source": [
        "Let's see how doc2query performs on TREC COVID:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9vGdlQccUEv"
      },
      "source": [
        "pt.Experiment(\n",
        "    [br, pt.BatchRetrieve(doc2query_indexref)],\n",
        "    topics,\n",
        "    qrels,\n",
        "    names=['DPH', 'doc2query'],\n",
        "    baseline=0,\n",
        "    eval_metrics=[AP(rel=2), nDCG, nDCG@10, P(rel=2)@10]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqcWcSmGcNdS"
      },
      "source": [
        "Similar to DeepCT, we see that the approach can significantly improve recall-oriented meausres, but doesn't help with precision-measures.\n",
        "\n",
        "Let's again investigate the top results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSJgkwb7cNAJ"
      },
      "source": [
        "pipeline = pt.BatchRetrieve(doc2query_indexref) % 1 >> pt.text.get_text(dataset, 'title')\n",
        "res = pipeline(topics)\n",
        "res.merge(qrels, how='left').head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB34igFldDni"
      },
      "source": [
        "Let's take a look at what queries it generates for some of these documents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asVAlbBSdGn3"
      },
      "source": [
        "df = pd.DataFrame(doc for doc in dataset.get_corpus_iter() if doc['docno'] in ('124czudi', 'gtp01rna'))\n",
        "df = df.rename(columns={'abstract': 'text'})\n",
        "doc2query_df = doc2query(df)\n",
        "for querygen, docno, text in zip(doc2query_df['querygen'], doc2query_df['docno'], df['text']):\n",
        "  print(docno)\n",
        "  print(querygen)\n",
        "  print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jo0T_hKdXMt"
      },
      "source": [
        "For \"*what causes death from covid 19*\" (gtp01rna), the top document focuses on the deaths from COVID in the US, but not on the specific causes due to COVID.\n",
        "\n",
        "For \"*how does the coronavirus respond to changes in weather*\" (124czudi), the top document is about climate change (similar to DeepCT)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW4_m3O4S34V"
      },
      "source": [
        "#  That's all folks\n",
        "\n",
        "If you aren't coming back for Part 4 of the tutorial, please don't forget to complete our exit quiz: https://forms.office.com/r/RiYSAxAKhk"
      ]
    }
  ]
}